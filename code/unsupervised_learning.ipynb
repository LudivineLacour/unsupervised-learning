{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Absenteeism at work \n",
    "\n",
    "Problem definition: predict the time of absence of an employee knowing some information on the reasons of abscence or the type of person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning \n",
    "\n",
    "**Goal:** Create cluster for columns that relate of employee personal caracteristics. \n",
    "\n",
    "**Methods:** I will use first Kmeans and may use another method such as Hierarchical clustering or DB Scan to compare the output of 2 different methods. \n",
    "\n",
    "**Unsupervised Learning tasks:** \n",
    "- [x] Define columns that can be gathered as a cluster \n",
    "- [x] Check distribution of data and relation between each other (apply scaling if necessary)\n",
    "- [x] Build model using Kmeans \n",
    "- [x] Plot the clusters to check the relevance of them\n",
    "- [x] Use Silouhette coefficient and Davies Bouldin evaluation metrics to check the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "pd.set_option('max_columns',25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/absenteeism_clean.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_cols = ['education','children','social_drinker','social_smoker','pet','weight',\n",
    "                'height','body_mass_index','age','distance_from_residence_to_work','transportation_expense']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[cluster_cols].drop(columns=['social_drinker','social_smoker']));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cluster_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building KMeans model\n",
    "model = KMeans(n_clusters=3)\n",
    "labels = model.fit_predict(df[cluster_cols])\n",
    "\n",
    "# Adding the label to the dataframe\n",
    "df_cluster['kmeans_label'] = labels\n",
    "\n",
    "# Number of unique id per cluster_label\n",
    "df_cluster.groupby('id').kmeans_label.min().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_cluster.id,df_cluster.kmeans_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the relation of features and clusters\n",
    "\n",
    "sns.pairplot(df_cluster[cluster_cols+['kmeans_label']],hue='kmeans_label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking accuracy of the cluster using evaluation metrics \n",
    "\n",
    "print('KMeans clusters')\n",
    "print(\"DB score\", davies_bouldin_score(df_cluster[cluster_cols], labels))\n",
    "print('Avg Silouhette coef:', silhouette_score(df_cluster[cluster_cols], labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on KMeans\n",
    "Here we can see the first approach is mostly based on transportation expenses because it is the column\n",
    "with the higher scale data. \n",
    "\n",
    "**Possible improvement:**\n",
    "- Apply a scaling method to weight equally the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________\n",
    "## KMeans model on scaled data\n",
    "\n",
    "- We will use the minmax scale method to reduce the weight of data with higher values\n",
    "- Build the model and compare how the clusters are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scale = ['weight','height','body_mass_index','age','distance_from_residence_to_work','transportation_expense']\n",
    "\n",
    "for i in range(len(minmax_scale)):\n",
    "    df_scaled[minmax_scale[i]] = (df_scaled[minmax_scale[i]] - df_scaled[minmax_scale[i]].min())/(df_scaled[minmax_scale[i]].max()- df_scaled[minmax_scale[i]].min())\n",
    "\n",
    "df_scaled[cluster_cols].head()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model on scaled dataset\n",
    "model_scaled = KMeans(n_clusters=3)\n",
    "labels_scaled = model_scaled.fit_predict(df_scaled[cluster_cols])\n",
    "\n",
    "# Adding the labels to dataframe\n",
    "df_cluster['kmeans_label_scaled'] = labels_scaled\n",
    "\n",
    "# Number of unique id per cluster_label\n",
    "df_cluster.groupby('id').kmeans_label_scaled.min().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the relance of clusters through visualization - WARNING! Takes time to run. \n",
    "\n",
    "sns.pairplot(df_cluster[cluster_cols+['kmeans_label_scaled']], kind=\"scatter\", hue='kmeans_label_scaled', \n",
    "             markers=[\"o\", \"s\", \"D\"], palette=\"Set2\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(4,3, figsize=(25,30))\n",
    "\n",
    "for i in range(df_cluster[cluster_cols].shape[1]):\n",
    "    tab = pd.crosstab(df_cluster[cluster_cols[i]],df_cluster['kmeans_label_scaled'], normalize='columns').round(2)\n",
    "    ax = axs[i//3,i%3]\n",
    "    sns.heatmap(tab,annot=True,ax=ax)\n",
    "\n",
    "fig.delaxes(axs[3,2])\n",
    "plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking accuracy of the cluster using evaluation metrics \n",
    "\n",
    "print('KMeans performance (scaled data)')\n",
    "print(\"DB score\", davies_bouldin_score(df_scaled[cluster_cols], labels_scaled))\n",
    "print('Avg Silouhette coef:', silhouette_score(df_scaled[cluster_cols], labels_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking evaluation metrics with non-scaled data - is it relevant?\n",
    "\n",
    "print('KMeans performance (scaled data) - comparison with non-scaled data')\n",
    "print(\"DB score\", davies_bouldin_score(df_cluster[cluster_cols], labels_scaled))\n",
    "print('Avg Silouhette coef:', silhouette_score(df_cluster[cluster_cols], labels_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions on having scaled data for KMeans\n",
    "\n",
    "In regards of the evaluation metrics we can see that the clusters are overlapping, which is not that good.\n",
    "\n",
    "But when checking visually the difference between clusters we can see it differenciates mostly with:\n",
    "- number of children (cluster 0 has mostly no children; cluster 1 has 1 or 2 children; cluster 3 has 2 or more children)\n",
    "- number of pet (cluster 0 has no pet; cluster 1 has mostly 2 or more pets; cluster 2 between 0 and 1)\n",
    "- age (majority of people in cluster 2 is around 28; cluster 1 is mostly more than 37 yo; cluster 0 is mostly between 31 and 36)\n",
    "- social drinker (cluster 0 is both drinkers and not; cluster 1 is mostly not social drinkers; cluster 2 is mostly social drinkers)\n",
    "\n",
    "Criteria of differentiation seem pretty relevant to me.\n",
    "\n",
    "**Possible improvements:**\n",
    "- Iterating on the number of clusters\n",
    "- Test using another method of clustering\n",
    "- Add train/test split to ensure model is not overfitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________\n",
    "## Kmeans iteration on clusters\n",
    "\n",
    "Assumptions: \n",
    "- We should iterate on the number of clusters by keeping in mind that we have only 36 uniques id so only 36 combinaisons of caracteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_n_cluster = list(range(2,15))\n",
    "\n",
    "df_it = df.copy()\n",
    "\n",
    "for n_cluster in range_n_cluster:\n",
    "    # Building the model\n",
    "    model = KMeans(n_clusters=n_cluster)\n",
    "    labels = model.fit_predict(df_scaled[cluster_cols])\n",
    "    \n",
    "    # Checking evaluation metrics\n",
    "    print(f'n_cluster = {n_cluster}')\n",
    "    print(\"DB score\", davies_bouldin_score(df_scaled[cluster_cols], labels))\n",
    "    print('Avg Silouhette coef:', silhouette_score(df_scaled[cluster_cols], labels))\n",
    "    \n",
    "    # Adding the cluster to a new dataframe to check the frequency per id\n",
    "    df_it[f'n_cluster_{n_cluster}'] = labels\n",
    "    \n",
    "    # Number of unique id per cluster_label\n",
    "    print(df_it.groupby('id')[f'n_cluster_{n_cluster}'].min().value_counts(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_it.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conlusions on KMeans iterations\n",
    "\n",
    "The more we add clusters the better the evaluation metrics but does it make sense to get so much clusters? \n",
    "\n",
    "As we have a limited number of id (people), maybe it is more relevant to keep a low number of clusters. \n",
    "\n",
    "**Possible improvements:**\n",
    "- Try DB Scan where the nb of clusters is not given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________\n",
    "## DB Scan\n",
    "\n",
    "Assumptions: \n",
    "- Because DB Scan is based on distance between datapoints we will keep the scaled dataframe to use this model\n",
    "- We want to find the ideal number of clusters so we use this model where that parameter is not given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in np.arange(0.5, 4, 0.5):\n",
    "    for min_samples in range(2,6):\n",
    "    \n",
    "        dbscan = DBSCAN(eps=eps,min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(df_scaled[cluster_cols])\n",
    "\n",
    "        # Checking accuracy of the cluster using evaluation metrics \n",
    "        print(f'DB Scan, eps={eps} & min_samples={min_samples}')\n",
    "        print('Count clusters', len(set(labels)))\n",
    "        print(\"DB score\", davies_bouldin_score(df_scaled[cluster_cols], labels))\n",
    "        print('Avg Silouhette coef:', silhouette_score(df_scaled[cluster_cols], labels),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=2, min_samples=2)\n",
    "labels = clustering.fit_predict(df_scaled[cluster_cols])\n",
    "\n",
    "# Checking accuracy of the cluster using evaluation metrics \n",
    "print('DB Scan model')\n",
    "print(\"DB score\", davies_bouldin_score(df_scaled[cluster_cols], labels))\n",
    "print('Avg Silouhette coef:', silhouette_score(df_scaled[cluster_cols], labels))\n",
    "\n",
    "# Adding the labels to dataframe\n",
    "df_cluster['dbscan_label'] = labels\n",
    "\n",
    "# Number of unique id per cluster_label\n",
    "df_cluster.groupby('id').dbscan_label.min().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions on DB Scan\n",
    "\n",
    "We can see here that the more we have clusters the better are the evaluation metrics (DB is low and Silouhette is high) but it seems not relevant to get so many clusters. \n",
    "\n",
    "Then, the output with low number of clusters (eps is 2 or 2.5) is not so relevant because thay are imbalanced. Most of the id are in one cluster and only few id are on the others.\n",
    "\n",
    "Finally, I would say that DB Scan is not relevant for our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________\n",
    "## Saving final dataframe with clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the new dataframe with the cluster\n",
    "\n",
    "df_final = df_cluster.drop(columns=cluster_cols+['cluster_label','dbscan_label']).copy().rename(columns={'kmean_label_scaled':'cluster'})\n",
    "print(df_final.shape)\n",
    "\n",
    "df_final.to_csv('../data/absenteeism_clusterized.csv', index=False)\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
