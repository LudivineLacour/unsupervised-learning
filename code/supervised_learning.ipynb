{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Absenteeism at work \n",
    "\n",
    "Problem definition: predict the time of absence of an employee knowing some information on the reason of absence or the type of person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised machine learning\n",
    "\n",
    "**Goal:** predict the time in hours of absenteisme. In the context of the problem we don't really need to get the time down to the minute but rather a global estimation of: is this employee going to be absent for 1/2 day or rather 2 days? \n",
    "\n",
    "**Type of supervised learning:** I will use classification model such as Decisiion Tree to get a prediction on the range of absenteeism. Decision Tree model will allow to deal with categorical data because I have many of them in the dataset.\n",
    "\n",
    "Though regression model would work on the target type of data, I will keep it as a way for improvement if classification models fail. \n",
    "\n",
    "**Preprocessing and modelling tasks:** \n",
    "- [x] Drop id column which is irrelevant for modelling\n",
    "- [x] Check types of columns an ensure the categorical data are well identified\n",
    "- [x] Check for multicollinearity and drop columns with high correlation\n",
    "- [x] Check distribution and choose the right scaling method\n",
    "- [x] Check for balance of dataset and over/under sampling if needed\n",
    "- [x] Create train/test samples \n",
    "- [x] Build Decision tree model \n",
    "- [x] Check performance of model using accuracy score, visualize confusion matrix using heatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/absenteeism_clusterized.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into features and target dataframes\n",
    "\n",
    "X = df.drop('absenteeism_bins',axis=1).copy()\n",
    "print(X.shape)\n",
    "\n",
    "y = df.absenteeism_bins\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Id columns because it has no impact on predicting the time of absence\n",
    "X.drop('id', axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical dtypes to object\n",
    "\n",
    "X[['reason_for_absence',\n",
    "   'month_of_absence',\n",
    "   'day_of_the_week',\n",
    "   'seasons','cluster']] = X[['reason_for_absence',\n",
    "                              'month_of_absence',\n",
    "                              'day_of_the_week',\n",
    "                              'seasons','cluster']].astype(object)\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dtype of target \n",
    "\n",
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking multicollinearity through data visualization\n",
    "\n",
    "sns.heatmap(abs(X.corr().round(2)), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking multicollinearity between numeric columns using VIF metrics\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n",
    "\n",
    "def drop_check_vif(column, X):\n",
    "    if column:\n",
    "        X=X.drop(column, axis=1)\n",
    "    vifs=pd.Series([VIF(X.values,i) for i in range(X.shape[1])],index=X.columns)\n",
    "    display(vifs[vifs>10])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of col to drop for multicollinearity (numeric columns)\n",
    "col_drop = []\n",
    "X_num = X._get_numeric_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_drop.append('hit_target')\n",
    "\n",
    "X_num = drop_check_vif(col_drop[-1], X_num)\n",
    "\n",
    "# I dropped 1 column with VIF above 10: hit_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with high multicollinearity\n",
    "\n",
    "X.drop(columns=col_drop[-1],inplace=True)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the frequency distribution of categorical features\n",
    "cat_features = X[X.columns[X.dtypes==object]]\n",
    "\n",
    "fig, axs=plt.subplots(2,3, figsize=(17,8))\n",
    "\n",
    "for i in range(cat_features.shape[1]):\n",
    "    ax=axs[i//3,i%3]\n",
    "    sns.distplot(cat_features.iloc[:,i],ax=ax)\n",
    "\n",
    "fig.delaxes(ax=axs[1,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the frequency distribution of numeric features\n",
    "fig, axs=plt.subplots(1,3, figsize=(17,4))\n",
    "\n",
    "for i in range(X_num.shape[1]):\n",
    "    sns.distplot(X_num.iloc[:,i],ax=axs[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardization because scale of values is the same for all\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_num_scaled = sc_X.fit_transform(X_num)\n",
    "\n",
    "X[X.columns[X.columns.isin(X_num.columns)]] = X_num_scaled\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking balance of dataset\n",
    "y.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on preprocessing\n",
    "\n",
    "We have imbalanced dataset regarding the frequency distribution of targets but this is normal because the categories with low frequency are outliers. \n",
    "\n",
    "Distribution of numeric features are not normally dstributed. We can assume that Decision Tree is not sensitive to normal distribution of data so we will keep them as it is. \n",
    "\n",
    "We can see that the categorical data are kinda uniformly distributed for seasons and day_of_week, so they may not affect much the model. \n",
    "\n",
    "We will build the model as it is and see for application of possible improvements afterwards.\n",
    "\n",
    "**Possible improvements:** \n",
    "- Apply over and under sampling methods to balance the dataset if imbalance of dataset has too much effect\n",
    "- Apply box-cox transformation if normallity would improve the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________\n",
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating samples for train and test data\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X,y,test_size = 0.3, random_state=42, stratify=y)\n",
    "\n",
    "print('Checking shape of samples')\n",
    "print('X_train',X_train.shape)\n",
    "print('X_test',X_test.shape,'\\n')\n",
    "print('Checking stratify of samples')\n",
    "print('y_train\\n',y_train.value_counts(normalize=True))\n",
    "print('y_test\\n', y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model\n",
    "dtree = DecisionTreeClassifier(random_state=8)\n",
    "dtree = dtree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dtree = dtree.predict(X_test)\n",
    "\n",
    "# Checking performance of model using evaluation metrics\n",
    "print(\"Accuracy score:\",accuracy_score(y_test,y_pred_dtree))\n",
    "\n",
    "# Checking overfitting of model by checking the accuracy of train sample\n",
    "y_train_pred = model.predict(X_train)\n",
    "print(\"Accuracy score for train sample:\",accuracy_score(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importance_dt = pd.DataFrame(model.feature_importances_, index=X.columns)\n",
    "feat_importance_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_score = accuracy_score(y_test,y_pred_dtree)\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test,y_pred_dtree, normalize='true').round(2), annot=True, xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.title(f'Normalized Confusion Matrix for Decision Tree\\n accuracy = {ac_score.round(4)}', fontsize=14)\n",
    "plt.savefig('../img/norm_confusion_matrix_decision_tree.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on Decision Tree\n",
    "\n",
    "The accuracy score is not so good and we can see there is an overfit of train sample, which can be frequent for Decision tree. \n",
    "\n",
    "In the stratify we clearly see the imbalance of target, we may want to correct that to see if there is improvement of accuracy (but we should keep in mind that the imbalance is due to outliers we may want to keep track on). \n",
    "\n",
    "**Possible improvements:**\n",
    "- Use crossvalidation to avoid overfitting\n",
    "- Test Random Forest which is better to handle overfitting\n",
    "- Under/over sampling dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________\n",
    "## Use Cross validation for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold # Use of Stratified to keep imbalanced samples\n",
    "\n",
    "list_of_accuracies=[]\n",
    "skf = StratifiedKFold(n_splits=4, random_state=8, shuffle=True) \n",
    "dtree_cv = DecisionTreeClassifier(random_state=8)\n",
    "\n",
    "for train_idx, test_idx in skf.split(X,y):\n",
    "    dtree_cv = dtree_cv.fit(X.iloc[train_idx,:],y[train_idx])\n",
    "    list_of_accuracies.append(accuracy_score(y[test_idx],dtree_cv.predict(X.iloc[test_idx,:])))\n",
    "    \n",
    "print(np.mean(list_of_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dtree_cv = DecisionTreeClassifier(random_state=8)\n",
    "csv = cross_val_score(dtree_cv, X, y, cv=4)\n",
    "print(csv)\n",
    "print(np.mean(csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion ofr cross-validation \n",
    "\n",
    "We can see there is no such improvement of the model accuracy using cross-validation.\n",
    "\n",
    "So we should use Random Forest to check the accuracy and make sure the overfitting is not the problem. Altough we may want to keep cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_accuracies=[]\n",
    "skf = StratifiedKFold(n_splits=4, random_state=8, shuffle=True) \n",
    "randomf = RandomForestClassifier(random_state=8)\n",
    "\n",
    "for train_idx, test_idx in skf.split(X,y):\n",
    "    randomf = randomf.fit(X.iloc[train_idx,:],y[train_idx])\n",
    "    list_of_accuracies.append(accuracy_score(y[test_idx],randomf.predict(X.iloc[test_idx,:])))\n",
    "    \n",
    "print(\"Average accuracy:\",np.mean(list_of_accuracies))\n",
    "list_of_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = randomf.predict(X_test)\n",
    "ac_score = accuracy_score(y_test,y_pred_rf)\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test,y_pred_rf, normalize='true').round(2), annot=True, xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('True labels')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.title(f'Normalized Confusion Matrix for Random Forest\\n accuracy = {ac_score.round(4)}', fontsize=14)\n",
    "plt.savefig('../img/norm_confusion_matrix_random_forest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importance_rf = pd.DataFrame(randomf.feature_importances_,index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importance = pd.merge(feat_importance_dt,feat_importance_rf, left_index=True, right_index=True, suffixes=('_dt','_rf'))\n",
    "feat_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on Random Forest\n",
    "\n",
    "Accuracy of model is much better with Random Forest so we can see it handle the overfitting of Decision tree model and the imbalanced of the dataset didn't affect too much the results. \n",
    "\n",
    "**Prossible improvements:**\n",
    "- Under/over sampling dataset to check if accuracy is better without keep outliers in low number\n",
    "- Test other decision-tree-like models such as Xgboost, Catboost and Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
